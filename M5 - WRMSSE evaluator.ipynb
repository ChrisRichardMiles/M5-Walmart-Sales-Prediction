{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Cross validation and the need for self evaluation\nIn order to understand how affective our method is in producing predictions that minimize the cost function (WRMSSE), we must develop cross validation strategies in which we make predictions on different validation datasets. In order to create different validation sets, we must have a tool that can accurately score our predictions in accordance with the WRMSSE. This is particularly difficult because the WRMSSE is based on weights and scaling factors that must be recalculated for every validation set. In this notebook, we will develop the tools to accurately implement the WRMSSE for any validation period with 28 days of know data. The last cell will be a WRMSSE object and helper functions that can easily used. \n\n# WRMSSE\n$$\nWRMSSE = \\sum_{i=1}^{42,840} \\left(W_i \\times \\sqrt{\\frac{\\sum_{j=1}^{28}{(D_j)^2}}{S_i}}\\right)\n$$\n* W_i: the weight of the ith series \n* S_i: the scaling factor of the ith series \n* D_j: The difference between sales and predicted sales for the ith series on day j\n\nI prefer to look at the equation like this: \n$$\nWRMSSE = \\sum_{i=1}^{42,840} \\frac{W_i}{\\sqrt{S_i}} \\times \\sqrt{\\sum_{j=1}^{28}{(D_j)^2}}\n$$\nTo build a WRMSSE scoring object, we will need to create tools that can apply this caclulation as efficiently as possible. We will develop a sparse aggregation matrix, created with a one-hot-encoding style, that serves to compute the aggregations for all 42840 series from the bottme level 30490 series. After the aggregation matrix, we will develop methods to compute the weights W and the scaling factor S for all series. We will then combine our tools to create a WRMSSE object, capable of scoring predictions for any 28 validation period of known data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom scipy.sparse import csr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Variables ####################\n# We will work through an example of calculating \n# the WRMSSE by level, and overall. Then we will \n# \nSTART_TEST = 1914 \nEND_TRAIN = START_TEST - 1 # last training day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Load data ###################\nDATA_PATH = '/kaggle/input/m5-forecasting-accuracy/'\ntrain_df = pd.read_csv(f'{DATA_PATH}sales_train_validation.csv')\nprices_df = pd.read_csv(f'{DATA_PATH}sell_prices.csv')\ncal_df = pd.read_csv(f'{DATA_PATH}calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#################### Dummy matrix #####################\n# We know we can compute all the aggregated series by \n# using matrix multiplication with the correctly \n# designed \"rollup\" matrix. Our daily sales have the \n# shape (number_items, prediction_horizon). Our rollup\n# matrix will need to have the shape \n# (number_series, number_items) so that we can execute \n# the matrix multiplication rollup x sales.\n\n# We need a list of the aggregating features that\n# will align with our weights and scales so that \n# our matrices will match up. Level 1 does not need\n# a column to group by.\n\n# For each sereis of each level of the WRMSSE, we will \n# use pandas get_dummies function on the corresponding\n# column or columns. \ndummy_matrices = [\n    pd.DataFrame({'all': np.ones((30490,)).astype('int8')}, index=train_df.index).T, \n    pd.get_dummies(train_df.state_id, dtype=np.int8).T,                                             \n    pd.get_dummies(train_df.store_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.cat_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.dept_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.state_id + '_' + train_df.cat_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.state_id + '_' + train_df.dept_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.store_id + '_' + train_df.cat_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.store_id + '_' + train_df.dept_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.item_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.item_id + '_' + train_df.state_id, dtype=np.int8).T,\n    pd.get_dummies(train_df.item_id + '_' + train_df.store_id, dtype=np.int8).T\n]\n\n# Take the transpose to correctly orient the matrix\nrollup_matrix = pd.concat(dummy_matrices, keys=range(1,13), names=['Level', 'id'])\n\n# Save the index for later use \nrollup_index = rollup_matrix.index\n\n# Sparse format will save space and calculation time\nrollup_matrix_csr = csr_matrix(rollup_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the weight of each series will be computed based on the last 28 observations of the training\nsample of the dataset, i.e., the cumulative actual dollar sales that each series displayed in that particular\nperiod (sum of units sold multiplied by their respective price)."},{"metadata":{"trusted":true},"cell_type":"code","source":"##################### Weights ########################\n# We need to convert the sales data into dollar sales \n# data so that we can correctly weight each series. \n\n# To begin, we consider only the last 28 days of \n# data before START_TEST. We then put the data into \n# \"long\" format so we can merge the calendar \n# and price information.\nd_cols = [f'd_{i}' for i in range(START_TEST - 28, START_TEST)]\ndf = train_df[['store_id', 'item_id'] + d_cols]\ndf = df.melt(id_vars=['store_id', 'item_id'],\n                       var_name='d', \n                       value_name = 'sales')\ndf = df.merge(cal_df[['d', 'wm_yr_wk']], on='d', how='left')\ndf = df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\ndf['dollar_sales'] = df.sales * df.sell_price\n\n# Now we will get the total dollar sales for each \n# item/store combination. Be sure to set sort=False\n# so that our index stays in the proper order. \n# We don't need df anymore\ndollar_sales = df.groupby(['store_id', 'item_id'], sort=False)['dollar_sales'].sum()\ndel df\n\n# We want to build a weight, scales,\n# and scaled weight columns \n# that are aligned with rollup_index. We \n# will divide dollar_sales by the total \n# dollar sales to get the weight W \n# for each series. We don't need dollar_sales anymore.\nw_df = pd.DataFrame(index = rollup_index)\nw_df['dollar_sales'] = rollup_matrix_csr * dollar_sales\nw_df['weight'] = w_df.dollar_sales / w_df.dollar_sales[0]\ndel w_df['dollar_sales']\n\n##################### Scaling factor #######################\n# We also need to calculate each series scaling factor S, \n# which is the denominator in the WRMSSE cacluation. It can \n# be pulled out of the square root and combined with the \n# series weight to make a single weight W/sqrt(S),\n# simplifying our calculations a bit. \n\n# S is the average squared difference of day to daily sales \n# for a series, excluding leading zeros, for all training \n# days leading up to START_TEST. \ndf = train_df.loc[:, 'd_1':f'd_{END_TRAIN}']\n\n# Aggregate all series, and replace leading \n# zeros with np.nan so that we can do numpy calculations \n# that will ignore the np.nan.\nagg_series = rollup_matrix_csr * df.values\nno_sale = np.cumsum(agg_series, axis=1) == 0\nagg_series = np.where(no_sale, np.nan, agg_series)\nscale = np.nanmean(np.diff(agg_series, axis=1) ** 2, axis=1)\n\n# Now we can finish our weights and scales dataframe by \n# adding scale and scaled_weight columns. \nw_df['scale'] = scale\nw_df['scaled_weight'] = w_df.weight / np.sqrt(w_df.scale)\nw_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## sample scoreing ##############\nactuals = train_df.iloc[:, -28:].values\npreds = np.ones((30490, 28)) \ndiff = actuals - preds\n\nnp.sum(\n    np.sqrt(\n        np.mean((rollup_matrix_csr * diff)**2, axis=1)\n           ) * w_df.scaled_weight\n      ) / 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################### Object oriented approach ######################\n# We will all of our tools into functions and utilize them in a WRMSSE \n# object. \n\n# Imports necessary for the functions and object\nimport pandas as pd \nimport numpy as np \nfrom scipy.sparse import csr_matrix\n\ndef get_rollup(train_df):\n    \"\"\"Gets a sparse roll up matrix for aggregation and \n    an index to align weights and scales.\"\"\"\n    \n    # Take the transpose to correctly orient the matrix\n    dummy_frames = [\n        pd.DataFrame({'all': np.ones((30490,)).astype('int8')}, index=train_df.index).T, \n        pd.get_dummies(train_df.state_id, dtype=np.int8).T,                                             \n        pd.get_dummies(train_df.store_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.state_id + '_' + train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.state_id + '_' + train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.store_id + '_' + train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.store_id + '_' + train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id + '_' + train_df.state_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id + '_' + train_df.store_id, dtype=np.int8).T\n    ]\n\n    rollup_matrix = pd.concat(dummy_frames, keys=range(1,13), names=['Level', 'id'])\n\n    # Save the index for later use \n    rollup_index = rollup_matrix.index\n\n    # Sparse format will save space and calculation time\n    rollup_matrix_csr = csr_matrix(rollup_matrix)\n    \n    return rollup_matrix_csr, rollup_index\n\n\n\ndef get_w_df(train_df, cal_df, prices_df, start_test, rollup_index): \n    \"\"\"Returns the weight, scale, and scaled weight of all series, \n    in a dataframe aligned with the rollup_index, created in get_rollup()\"\"\"\n    \n    d_cols = [f'd_{i}' for i in range(start_test - 28, start_test)]\n    df = train_df[['store_id', 'item_id'] + d_cols]\n    df = df.melt(id_vars=['store_id', 'item_id'],\n                           var_name='d', \n                           value_name = 'sales')\n    df = df.merge(cal_df[['d', 'wm_yr_wk']], on='d', how='left')\n    df = df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n    df['dollar_sales'] = df.sales * df.sell_price\n\n    # Now we will get the total dollar sales \n    dollar_sales = df.groupby(['store_id', 'item_id'], sort=False)['dollar_sales'].sum()\n    del df\n\n    # Build a weight, scales, and scaled weight columns \n    # that are aligned with rollup_index. \n    w_df = pd.DataFrame(index = rollup_index)\n    w_df['dollar_sales'] = rollup_matrix_csr * dollar_sales\n    w_df['weight'] = w_df.dollar_sales / w_df.dollar_sales[0]\n    del w_df['dollar_sales']\n\n    ##################### Scaling factor #######################\n    \n    df = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n    agg_series = rollup_matrix_csr * df.values\n    no_sale = np.cumsum(agg_series, axis=1) == 0\n    agg_series = np.where(no_sale, np.nan, agg_series)\n    scale = np.nanmean(np.diff(agg_series, axis=1) ** 2, axis=1)\n\n    w_df['scale'] = scale\n    w_df['scaled_weight'] = w_df.weight / np.sqrt(w_df.scale)\n    \n    return w_df\n\n\n####################### WRMSSE object ##########################\nclass WRMSSE():\n    \"\"\"An object that is capable of scoring predictions for any \n    time period, provied the necessary dataframes.\"\"\"\n    def __init__(self, train_df, cal_df, prices_df, start_test): \n        self.train_df = train_df\n        self.cal_df = cal_df\n        self.prices_df = prices_df \n        self.start_test = start_test\n        self.actuals = train_df.loc[:, f'd_{self.start_test - 28}': f'd_{self.start_test - 1}'].values\n        \n        self.rollup_index, self.rollup_matrix_csr = get_rollup(self.train_df)\n        self.w_df = get_w_df(self.train_df, self.cal_df, self.prices_df, self.start_test)\n        \n        self.level_scores = []\n        \n    def score(self, preds): \n        if type(preds) == pd.DataFrame: \n            preds = preds.values\n        diff = self.actuals - preds\n        res = np.sum(\n                np.sqrt(\n                    np.mean((self.rollup_matrix_csr * diff)**2, axis=1)\n                       ) * self.w_df.scaled_weight\n                  ) / 12\n        return res","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}